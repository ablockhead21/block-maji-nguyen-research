\section{Preliminaries}\label{sec:prelims}
\textcolor{red}{As a note, we shall keep these preliminaries general so that they apply to each section below. Any additional background will be presented on a per section basis.}

We let $\lg$ denote the binary logarithm.
We denote random variables and distributions by capital letters, for example $X$, and the values taken by small letters, for example $X = x$ or $x \drawn X$.
For a positive integer $n$, we write $[n]$ and $[-n]$ to denote the sets $\{ 1,\dotsc, n \}$ and $\{ -n,\dotsc, -1 \}$, respectively.
We consider the field $\bbF = \GF{q}$, where $q = p^a$ for prime $p$ and positive integer $a$.
We let $c\sub{n}$ denote the vector $(c_1,\dotsc, c_n) \in \bbF^n$.
For any $c\sub{n} \in \bbF^n$, define the function \wt{c\sub{n}} as the cardinality of the set $\{ i \colon c_i \neq 0 \}$.
We let $\cS_n$ denote the set of all permutation $\pi \colon [n] \rightarrow [n]$.
For any two $x\sub{n}, y\sub{n} \in \bbF^n$, we let $\ip{x\sub{n}}{y\sub{n}} \in \bbF$ denote the inner-product of $x\sub{n}$ and $y\sub{n}$, and let $x\sub{n}*y\sub{n} \in \bbF^n$ denote the point-wise (Schur) product of $x\sub{n}$ and $y\sub{n}$.
For any vector $c\sub{n} \in \bbF^n$ and permutation $\pi \in \cS_n$, we define $\pi(c\sub{n})\defeq (x_{\pi(1)},\dotsc, x_{\pi(n)})$.
We also denote sets by capital letters, and given a set $Y$ we let $U_Y$ denote the uniform distribution on the set $Y$.
We denote sampling $y$ according to $U_Y$ as $y \getsr Y$.


\subsection{Functionalities and Correlations}\label{sec:prelim-func-corr}
We define some useful functionalities and correlations.

\paragraph{Oblivious Transfer.}
Oblivious transfer, denoted by \OT, is a two-party functionality which takes $(m_0,m_1) \in \zo^2$ as input from Alice and $c \in \zo$ as input from Bob and outputs $m_c$ to Bob.

\paragraph{Oblivious Linear-function Evaluation.}
For a field $\bbF$, oblivious linear-function evaluation over $\bbF$, denoted by \OLE[\bbF], is a two-party functionality which takes $(a,b) \in \bbF^2$ as input from Alice and $x \in \bbF$ as input from Bob, and outputs $z = ax+b$ to Bob.
In particular, \OLE[\bbF] is a natural extension of \OT to any finite field, and \OLE[\GF{2}] is functionally equivalent to \OT by the following observation: $m_b = (m_1 - m_0)b + m_0$.

\paragraph{Random Oblivious Transfer Correlation.}
The random oblivious transfer correlation, denoted by \ROT, is a correlation that samples $m_0,m_1,c \getsr \zo$ uniformly and independently at random and provides secret shares $r_A = (m_0,m_1)$ and $r_B = (c,m_c)$ to Alice and Bob, respectively.

\paragraph{Random Oblivious Linear-function Evaluation Correlation.}
For a field $\bbF$, the random oblivious-linear function evaluation correlation over $\bbF$, denoted by \ROLE[\bbF], is a correlation which that samples $a,b,x \getsr \bbF$ uniformly and independently at random, and provides secret shares $r_A = (a,b)$ and $r_B = (x, z \defeq ax+b)$ to Alice and Bob, respectively.
In particular, we let \ROLE denote the \ROLE[\GF{2}] correlation, and note that \ROT and \ROLE are functionally equivalent correlations.

\paragraph{Inner-product Correlation.}
For a field $\bbF$ and $n \in \bbN$, the inner-product correlation over $\bbF$ of size $n$, denoted by \IP[\bbF^n], is a correlation that samples random $r_A = (x_0,\dotsc, x_{n-1}) \in \bbF^n$ and $r_B = (y_0,\dotsc, y_{n-1}) \in \bbF^n$ subject to the constraint that $x_0 + y_0 = \sum_{i=1}^{n-1} x_iy_i$.
The secret shares of Alice and Bob are $r_A$ and $r_B$, respectively.

For $m \in \bbN$, the functionality or correlation $\cF^m$ represents the functionality that implements $m$ independent copies of any functionality or correlation $\cF$.

\subsection{Correlation Extractors}\label{sec:prelim-corr-ext}
We denote the correlated private randomness by $(R_A,R_B)$ and the $t$-leaky version of $(R_A,R_B)$ by $\leaky{R_A,R_B}{t}$.
Recall \defref{corr-ext} for the definition of an $(n,m,t,\eps)$-correlation extractor.
We define the {\em production rate} of a correlation extractor as $m/n$, and the {\em leakage rate} as $t/n$.

\paragraph{Leakage Model.} We define our leakage model for a correlation $(R_A,R_B)$ as follows.
\begin{enumerate}
	\item {\bfseries Correlation generation phase.} Alice receives $r_A$ and Bob receives $r_B$ such that $(r_A, r_B) \drawn (R_A,R_B)$ and the size of the secret share of each party is $n$-bits.
	
	\item {\bfseries Corruption and leakage phase.} A semi-honest adversary corrupts either the sender (Alice) or receiver (Bob).
	If the sender is corrupted, the adversary sends a leakage function $L \colon R_B \rightarrow \zo^t$ and receives $L(r_B)$.
	If the receiver is corrupted, the adversary sends a leakage function $L\colon R_A \rightarrow \zo^t$ and receives $L(r_A)$.
	We emphasize that this leakage need not be on the individual bits of the shares $r_A$ or $r_B$, but on the entire share.
	Thus this leakage can encode crucial global information.
\end{enumerate}

\paragraph{Correctness.} The correctness condition states that the output of the receiver is correct in all $m/2$ instances of $\ROT$.

\paragraph{Privacy.} The privacy requirement states the following.
Let $(s_0\p{i}, s_1\p{i})$ be the output shares of Alice and let $(c\p{i}, z\p{i})$ be the output shares of Bob in the $i^th$ instance of \ROT.
A corrupt sender (respectively, receiver) cannot distinguish between $\{ c\p{i} \}_{i \in [m/2]}$ (respectively, $\{ s_{1-\p{i}}\p{i}  \}_{i \in [m/2]}$) and $r \getsr \zo^{m/2}$ with advantage more than $\eps$.

\subsection{Fourier Analysis over Finite Fields}\label{sec:prelim-fourier}

\subsection{Distributions and Min-Entropy}\label{sec:prelim-min-ent}
For a probability distribution $X$ over sample space $U$, the entropy of $x \in X$ is defined as $H_X(x) = -\lg\Pr[X = x]$.
The min-entropy of $X$, denoted by $\minentropy(X)$, is defined to be $\min_{x \in \supp(X)} H_X(x)$.
The binary entropy function, denoted by $\binent(x)$, is defined to be $-x \lg x - (1-x) \lg(1-x)$ for every $x \in (0,1)$.

For a joint distribution $(X,Y)$ over sample space $U \times V$, the marginal distribution $Y$ is a distribution over sample space $V$ such that for every $y \in V$, $\Pr[y] = \sum_{x \in U} \Pr[X = x, Y = y]$.
The conditional distribution $(X|y)$ is the distribution over sample space $U$ such that for every $x \in U$, $\Pr[x] = \Pr[X = x | Y = y]$.
The average min-entropy \cite{DORS08}, denoted by $\ame(X|Y)$, is defined to be $-\E_{y \drawn Y}[2^{-\minentropy(X|y)}]$.

\subsubsection{Family of Small-Bias Distributions}\label{sec:prelim-small-bias}

\subsubsection{Distributions over Linear Codes}\label{sec:prelim-codes}





