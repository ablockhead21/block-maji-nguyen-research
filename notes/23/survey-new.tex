\section{Learning Parity with Noise.}
We discuss results related to the Learning Parity with Noise problem.
Formally, there are two ways to define hardness of the \LPN problem: the so called {\em search} and {\em decision} \LPN problems.

\begin{definition}[Learning Parity with Noise Problem]
	For $0 < \tau < \frac{1}{2}$ and $n \in \bbN$, the $\LPN_{\tau, k}$ problem samples secret $\bs \getsr \bbF_2^k$, matrix $\bA \getsr \bbF_2^{k \times n}$, and $\be \getsr \pred{Ber}_\tau^{n}$ and outputs $(\bA, \bs\bA\oplus \be)$.
	
	The {\em Search} $\LPN$ assumption states that $\LPN_{\tau, k}$ is $(n, t, \eps)$-hard if for every machine $\pred{M}$ running in time $t$,
	\begin{align*}
		\underset{\bs,\bA,\be}{\Pr}\left[\pred{M}(\bA,\bs^\top\bA\oplus\be)=\bs\right] \leq \eps.
	\end{align*}
	
	The {\em Decisional} $\LPN$ assumption states that $\LPN_{\tau, k}$ is $(n, t, \eps)$-hard if for every distinguisher $\pred{D}$ running in time $t$,
	\begin{align*}
		\abs{\underset{\bs,\bA,\be}{\Pr}\left[\pred{D}(\bA,\bs^\top\bA\oplus\be)=1\right] - \underset{\br, \bA}{\Pr}\left[\pred{D}(\bA,\br)=1\right] }\leq \eps,
	\end{align*}
	where $\br \getsr \bbF_2^n$.
\end{definition}
The first hardness assumption states that given an \LPN sample, it is computationally difficult to output the original secret $\bs$.
The second hardness assumption simply states that \LPN samples are pseduo-random.

The \LPN problem can also be formulated as the problem of decoding (noisy) random linear codes \cite{applebaum2008fast}.
\begin{definition}[Decoding Random Linear Code Problem {\cite{applebaum2008fast}}]
	Let $n = n(k)$ be the code length parameter and $0 < \tau < \frac{1}{2}$ be a noise parameter.
	Consider the following ``decoding game''.
	\begin{boxedalgo}
		$\decode(n,\tau)$:
		\vspace*{-8pt}
		\begin{enumerate}
			\itemsep-2pt
			\item Pick $\bA \getsr \bbF_2^{k\times n}$
			\item Pick $\bs \getsr \bbF_2^{k}$
			\item Encode $\by = \bs^\top\bA$
			\item Output $\hat{\by}$, where $\hat{\by}$ is obtained by independently flipping each bit of $\by$ with probability $\tau$.
		\end{enumerate}
	Given $\hat{\by}$, an adversary \cA wins the game if it outputs $\bs$.
	\end{boxedalgo}

	The {\em Hardness of Decoding Random Linear Code} assumption states that for every constant $0 < \tau < \frac{1}{2}$ and polynomial $n(\cdot)$, any PPT adversary $\cA$ wins $\decode(n,\tau)$ with probability no more than $\eps = \negl(k)$. 
\end{definition}

\subsection{Evidence of Hardness}\label{sec:hardness}

The following results give support for the plausibility of these hardness assumptions.
Algorithms for both the \LPN and decoding random linear code problems are given.
For all algorithms, we assume $k$-bit secrets, noise parameter $\tau$, and output length $n$.


\paragraph{The BKW Algorithm.} Blum \etal \cite{STOC:BluKalWas00} give an algorithm for noise rate $\tau < \frac{1}{2} - 2^{-k^\delta}$, for any constant $\delta < 1$, which solves the \LPN problem in time and sample complexity $2^{O(k/\log k)}$.
The basic idea of the algorithm is as follows (restated from the description provided in \cite[Section 2]{Levieil06}).

The algorithm gets a large \LPN sample $(\bA, \by = \bs^\top\bA \oplus \be)$ such that $\bA \in \bbF_2^{k\times n}$ with $n = 2^{O(k/\log k)}$.
Next the algorithm carefully picks $2^a = O(k/\log k)$ columns of $\bA$ and computes their sum such that the sum results in some basis vector $e_j$.
Let $\cI = {i_1, \dotsc, i_{2^a}}$ be the indices of the columns chosen.
The algorithm then computes
\begin{align*}
	\bigoplus_{i \in \cI} \by_i &= \ip{\bs}{\bigoplus_{i \in \cI} \bA_i}\oplus \bigoplus_{i \in \cI}\be_i=\ip{\bs}{e_j} \oplus \bigoplus_{i \in \cI}\be_i =\bs_j \oplus \bigoplus_{i \in \cI}\be_i.
\end{align*} 
Note here $\bA_i$ is the $i$-th column of the matrix $\bA$, and $\be_i$ is the $i$-th bit of the noise vector $\be$.
Note also that the sum $\bigoplus_{i \in \cI}\be_i$ has less bias than the individual bits of $\be_i$ (cf. \cite[Lemma 4]{STOC:BluKalWas00}).
However, since $|\cI|$ is small, the bias is not too small.
Thus taking enough independent combinations of columns of $\bA$ which sum to $e_j$ and taking majority, with high probability we can compute the correct value of $\bs_j$.

The algorithm does the above process recursively by first splitting the columns into $a$ blocks of size $b = \ceil{\frac{k}{a}}$ bits each.
Recursively, starting with the last block and continuing until the second, use the $b$ bits of the current block to partition the columns into equivalence classes.
For each class, the algorithm chooses a vector at random, computes the vector sum of this vector with all others in the class, then removes the chosen vector from the class.
Then at the end of this step, all bits in the current block are zero.
After repeating this process recursively, if there are enough equations left, we can take majority vote which gives information about $\bs_j$ with high probability, and repeat for different $j$ to recover $\bs$.

%Using a large number of vectors in $\bbF_2^k$ (\ie, $n$ is large), carefully pick a few vectors and compute the vector sum such that the sum results in some basis vector $e_j$.
%For parameter $a$, the algorithm finds $2^a = O(k/\log k)$ vectors $\ba_{i_1},\dotsc, \ba_{i_{2^a}}$ such that
%\begin{align*}
%	\ba_{i_1} \xor\cdots\xor \ba_{i_{2^a}} = e_j.
%\end{align*}
%Note that these samples are erroneous, so there is a blow-up in the noise (cf. \cite[Lemma 4]{STOC:BluKalWas00}).
%However, since the number of samples is small, the blow-up is not too much.
%Therefore with enough independent combinations which equal $e_j$, taking majority allows us to recover $\bs_j$.
%
%Assuming $b = \ceil{\frac{k}{a}}$ and $k = ab$, the algorithm splits the $k$ bits of each sample $\ba_i$ into $a$ blocks of $b$ bits.
%Next according to the last $b$ bits, compute $2^b$ equivalence classes and classify each $\ba_i$ according to these classes.
%For each class, the algorithm chooses a random vector, performs the XOR with all other vectors of the same class, then removes the randomly chosen vector from the class.
%Therefore the last $b$ bits of every class are zero.
%
%The procedure is called recursively from the last block until the second block.
%After, there are enough equations such that $e_j$ is recovered and by majority gives the value $\bs_j$ with high probability.
%Applying for each $j$ gives the result.

\paragraph{Modified BKW Algorithm.} \cite{Lyubashevsky05} gives an significant improvement to the query complexity of the BKW Algorithm at the cost of a slight loss in running time and error tolerance.
In particular, they show that using $k^{1+\eps}$ samples (\ie, $n = k^{1+\eps}$), their variation of the BKW algorithm runs in time $2^{O(k/\log\log k)}$, given the noise rate $\tau < \frac{1}{2} - 2^{-(\log k)^\delta}$, for any constant $\delta < 1$, and any $\eps > 0$.
This algorithm also gives a sub-exponential algorithm for decoding $k\times k^{1+\eps}$ random binary linear codes with noise, and an extension of the techniques used gives a sub-exponential algorithm for dense instances of the random subset sum problem.
\textcolor{red}{The author notes that the techniques in this result do not allow one to extend the algorithm to decoding random $k \times \alpha k$ random binary linear codes for constant $\alpha > 1$, which is left as an open problem.}
%\TODO{Check if this is still open.}

The main idea of this algorithm is to use the original BKW algorithm as a black box, and build a function to provide the large number of samples needed for the BKW algorithm.
The key idea is given $k^{1+\eps}$ pairs $(\ba_i, \bl_i)$, for $\ba_i \getsr \bbF_2^k$ and $\bl_i = \ip{\ba_i}{\bs} +\be_i$ for secret $\bs\getsr \bbF_2^k$ (\ie, the output $(\bA, \bs^\top\bA\xor \be)$), the pairs are treated as a random element from a certain family of functions which maps elements from some domain $X$ to some labeled examples (\ie, samples).
More precisely, given $(\ba_1,\bl_1),\dotsc,(\ba_{k^{1+\eps}}, \bl_{k^{1+\eps}})$ and set $X = \{ \bx \in \bbF_2^{k^{1+\eps}}~\colon~\sum_ix_i = \ceil{\frac{2k}{\eps\log k}} \}$, the function selects a random $\bx \in X$ and outputs
\begin{align*}
	\ba' = \bigoplus \bx_i\ba_i & & \bl' = \bigoplus \bx_i\bl_i,
\end{align*}
and the generation of these $\ba'$ and $\bl'$ are shown to be statistically close to uniform, so they are nearly identical to more random instances of \LPN.

\subsubsection{Hardness Results Table}
The following table is a quick summary of the best algorithms for solving \LPN.

\input{fig-lpn-alg-table}

\subsection{Results using LPN and its Variants}
We discuss various results using the \LPN assumption and its variants.
To begin, we note that both \LPN assumptions (namely, search and decisional) are polynomially equivalent \cite{C:BFKL93,JC:KatShiSmi10}, as given by the following lemma.

\begin{importedlemma}[{\cite[Lemma 1]{JC:KatShiSmi10}}]\label{implem:lpn-poly}
	If decisional $\LPN_{\tau, k}$ is not $(n,t,\eps)$-secure, then search $\LPN_{\tau,k}$ is not $O(n',t',\eps')$ secure, where
	\begin{align*}
	n' = O(n\cdot \log k/\eps^2) & & t' = O(t\cdot k \cdot \log k/\eps^2) & & \eps' = \eps/4.
	\end{align*}
\end{importedlemma}

In particular, \importedlemmaref{lpn-poly} allows us to use either search or decisional \LPN as a hardness assumption.

\subsubsection{Randomness Requirements of LPN}
The most expensive cost of generating instances of \LPN is the generation of the matrix \bA, which is assumed to be uniformly random over $\bbF_2^{k\times n}$.
As an example, if $n = O(k)$, then we need $O(k^2)$ random bits to generate the matrix.
\cite{EC:GilRobSeu08} suggest using Toeplitz matrices to generate $\bA$ in place of a uniformly random matrix, reducing the randomness needed from $k\cdot n$ to $k + n$.
\textcolor{red}{However, it is not known if the \LPN problem remains hard if $\bA$ is replaced with a Toeplitz matrix (though it seems plausible).
Other randomness saving techniques may be possible as well, such as sampling bits of \bA from high-min entropy sources rather than uniformly random.}

The secret $\bs$ of \LPN is also assumed to be drawn uniformly at random.
\cite{C:ACPS09} show that sampling \bs from $\pred{Ber}_\tau^k$ is also sufficient for \LPN (\ie, \LPN with $\pred{Ber}_\tau^k$ secrets is as hard as with uniform secrets).
Here is another place with potential to save randomness.
In particular, if \bs is sampled from a {\em vector subspace} $V \subseteq \bbF_2^k$ of dimension $k'$, where $k' \leq k$, then this instance of \LPN is at most as hard as \LPN with secret length $k'$.
Formally, this instance of \LPN is known as {\em subspace} \LPN \cite{TCC:Pietrzak12}, and is an interactive assumption.
Let $\phi_a$ and $\phi_s$ be two affine functions over $\bbF_2^k$. 
That is, $\phi_a(\by) = \by^\top\bX_a \oplus \bx_a$ and $\phi_s(\by) = \by^\top\bX_s \oplus \bs$, where $\bX_a,\bX_s \in \bbF_2^{k \times k}$ and $\bx_a,\bx_s \in \bbF_2^k$.
The subspace $\LPN_{\tau, k}$ assumption states that the \LPN problem is hard even when an adversary adaptively chooses $\phi_s$ and $\phi_a$ and receives $\ip{\phi_a(\bA_i)}{\phi_s(\bs)}\oplus \be_i$, where $\bA_i$ is the $i^{th}$ row of $\bA$, in addition to the samples $\bs^\top\bA \oplus \be$.
\cite{TCC:Pietrzak12} shows that if $\bX_a^\top \cdot \bX_s$ has rank at least $k' \leq k$, then the subspace $\LPN_{\tau,k}$ problem is at most as hard as the standard $\LPN_{\tau, k'}$ problem with secret length $k'$.

The subspace \LPN problem is a particular instance where the secret $\bs$ is chosen from a min-entropy source.
In the case of subspace \LPN, the secret $\bs$ has min-entropy $k'$ since it is equivalent to $\LPN$ with secret length $k' \leq k$.
\textcolor{red}{It is an open problem whether the \LPN problem remains hard when sampling \bs from general min-entropy sources.
Such a result would also save some randomness and allow more versatility.}

For non-uniform noise, there are a few negative results.
For example, if $\ell$ positions of the noise vector $\be$ are fixed, then an adversary knows $\ell$ linear equations without noise $\ip{\ba}{\bs} = y$ and can compute $\bs$ from these equations using Gaussian elimination.
There is also an attack due to \cite{ICALP:AroGe11} where the bits of $\be$ are not i.i.d., but are sampled in the following manner.
For some $m$, the noise vector $e \in \bbF_2^n$ is sampled $m$-bits at a time.
Each $m$-bit block is sampled independently, with the condition that this block has at most $w = m \cdot \tau$ positions which are $1$.
Thus $\be$ is distributed according to $\Ber_\tau^n$, but noise bits are no longer independent.
\cite{ICALP:AroGe11} then use ``linearization'' to show one can recover the secret in time $n^w$.
\textcolor{red}{Though it seems to be difficult to adjust the noise distribution, are there any other noise distributions for which \LPN remains hard?}

\subsubsection{Cryptographic Primitives from LPN}
We outline constructions of cryptographic primitives from the \LPN assumption.
We present these constructions \visavis other constructions.
As noted in \cite{TCC:Pietrzak12}, the main appeal of using \LPN is the fact that \LPN based schemes can be extremely efficient, only requiring relatively few bit operations to compute an inner product of two bit-vectors.

\paragraph{One-way Functions.} We outline the construction of one-way functions from search \LPN.
Fix matrix $\bA\getsr \bbF_2^{k\times n}$.
Define function $f_\bA \colon \zo^* \rightarrow \zo^*$ as follows.
On input $\bx$ which is sufficiently long, use $\bx$ to sample $\bs \getsr \bbF_2^k$ and $\be \drawn \Ber_\tau^n$.
If $\wt(\be)\geq k\cdot \frac{1/2+\tau}{2}$, output $\bot$.
Note $\bot$ is output with exponentially small probability by Chernoff bound.
Otherwise, output $(\bA, \bs^\top\bA\oplus\be)$.
Note that $f$ is one-way since clearly it is poly-time computable (assuming sampling of $\bs$ and $\be$ can be done in polynomial time) and any algorithm which can easily invert $f_\bA(\bx)$ must find the unique secret $\bs$, contradicting the \LPN assumption.

The construction of a one-way function in this manner invokes the following costs.
First, there is a one-time cost of generating the matrix $\bA$.
Next any input $\bx$ must be sufficiently long so that it can be used to generate $\bs$ and $\be$.
Since $\be$ has $n\cdot h_2(\tau)$ bits of entropy, $\bx$ must have length at least $k + nh_2(\tau)$.
Finally, the size of the output is $(k+1)n$ bits long.
Assuming there are efficient ways to sample $\bs$ and $\be$, the cost of computing the one-way function is the cost of sampling $\bs$ and $\be$ and the cost of the matrix multiplication $\bs^\top\bA$, which is $O(kn)$ bit-level multiplications.



\paragraph{Pseudo-random Generators.} The construction of PRGs from \LPN follows directly from the decisional \LPN assumption.
Namely, the decisional \LPN assumption states that \LPN samples are pseudorandom.
Therefore \LPN yields a simple PRG \cite{C:BFKL93}.
We discuss the construction and efficiency of this PRG.

The PRG is defined as follows. 
On input $\bx$, use $\bx$ to sample $\bA \getsr \bbF_2^{k\times n}$, $\bs \getsr \bbF_2^k$, and $\be \drawn \Ber_\tau^n$, and output $(\bA, \bs^\top\bA\oplus\be)$.
The randomness requirements currently are high, but a crucial observation by \cite{C:ACPS09} is that the matrix $\bA$ can be fixed as a public parameter.
So there is a one-time cost of computing $\bA$ and it no longer needs to be sampled or output by the PRG.
Therefore on input $\bx$, the PRG uses $\bx$ to sample $\bs$ and $\be$ and outputs the $n$-bit string $\bs^\top\bA\oplus\be$.

For $\tau < \frac{1}{2}$, $k \in \bbZ$, and sufficiently large $n$, we use seed $\bx$ to construct the PRG, where $\mathrm{len}(\bx) = x < n$.
To sample $\bs \getsr \bbF_2^k$, $k$ bits are needed.
To sample $\be\drawn\Ber_\tau^n$, we note again that $\be$ has $nh_2(\tau)$ bits of entropy, since each bit of $\be$ has $h_2(\tau)$-bits of entropy.
As noted in \cite{C:ACPS09}, efficient sampling of $\be$ is possible using roughly the same number of bits $nh_2(\tau)$.
Therefore the pair $(\bs,\be)$ has $k+nh_2(\tau)$ bits of entropy, where $r\defeq k+nh_2(\tau) < n$ for sufficiently large $n$.
This construction yields a PRG with stretch $(1 - h_2(\tau))n-k$, which is linear in the length of the seed $r = k+nh_2(\tau)$.

To summarize the construction of a PRG from the decisional \LPN assumption, we note the following.
\begin{itemize}
	\item The running time of this PRG is $O(k\cdot n)$; that is, it is dominated by the matrix multiplication of $\bA$ with $\bs$.
	Here, $n = n(k)$, so for values of $n = \poly(k)$, this algorithm is ``efficient''.
	As noted in \figureref{lpn-alg-table}, even for $n = \poly(k)$, the best algorithms known run in time $2^{O(k/\log\log k)}$, so this PRG is efficient to evaluate in time $O(k\cdot\poly(k))$ while there's no efficient algorithm to break \LPN.
	\item The matrix $\bA$ can be set as a public parameter, so there is no need to sample this matrix or include it in the output.
	\item Any seed of length $r = k + nh_2(\tau)$ yields a linear stretch PRG $G \colon \zo^r \rightarrow \zo^n$, with stretch $(1-h_2(\tau))n - k$, where $\tau$ is the noise parameter of the \LPN problem.
\end{itemize}
It is also worth noting that construction of PRGs via \LPN is a direct construction, and avoids having to go through the route of one-way functions for their construction.
In particular, though \LPN implies one-way functions, the construction of an \LPN one-way function is not as elegant as the direct construction of a PRG from decisional \LPN.


\paragraph{Pseudo-random Functions.} \cite{C:YuZha16} give parallelizable constructions of randomized PRFs from \LPN with noise rate $\tau = O(k^{-c})$ for any constant $0 < c < 1$.
Further, these PRFs can be implemented with a poly-size circuit family with unbounded fain-in AND, OR, and XOR gates of depth $\omega(1)$ (any small super constant).

Slightly more formally, they prove the following: suppose the decisional $\LPN_{\tau,k}$ problem is $[n=\alpha k, t=2^{O(k^{1-c})}, \eps=2^{-O(k^{1-c})}]$-hard for $\tau = k^{-c}$, where $0 < c < 1$ and $\alpha > 1$ are any constants.
Then, there is a tradeoff between the following:
\begin{enumerate}
	\item For any $d = \omega(1)$, there exists a family of randomized PRFs on weak keys of R\'{e}nyi entropy at least $O(k^{1-c}\log k)$ such that there exists no adversary which distinguishes the function with more than $O(k^{1+d/3}\eps)$ probability, running in $t-k^{d/3}\poly(k)$ time and $k^{d/3}$ queries.
	
	\item For $\lambda = \Theta(k^{1-c}\log k)$ and any $d = \omega(1)$, there exists a PRF family of key length $\lambda$ such that there exists no adversary which distinguishes the function with more than $2^{-O(\lambda/\log \lambda)}$ probability, running in $2^{O(\lambda/\log\lambda)}$ time and $\lambda^{\Theta(d)}$ queries.
\end{enumerate}
Both of the above PRFs are computable by a polynomial-size depth $O(d)$ circuit with unbounded fan-in AND, OR, and XOR gates.
Their construction is security-preserving which does not rely on $k$-wise independent hashing.

\paragraph{Secret Key Encryption.} Given that we can construct a PRG from the \LPN assumption, it is possible to use the PRG to give a secret encryption scheme.
However, we need the following property to hold: for an \LPN PRG $G \colon \zo^r \rightarrow \zo^n$ and vector $\br \getsr \zo^r$, evaluating $G(\br)$ twice yields the same value.
If this holds, then $\br$ can be a shared secret key, and to encrypt an $n$-bit message $m$, we simply use $G(\br)$ as a one-time pad.
Since the PRG construction uses $\br$ to sample $\bs \getsr \zo^k$ and $\be \drawn \Ber_\tau^n$, this may not be the case.
Further, even if the restriction is lightened to only insist that $\bs$ is the same when evaluating the same $\br$ multiple times, the encryption scheme introduces errors.

This motivates the construction of a secret key encryption scheme directly from \LPN instead of using the \LPN PRG.
\cite{Piet12} describes a modified version of the secret key encryption scheme of \cite{ICALP:GilRobSeu08} using \LPN.
Suppose $\bm \in \bbF_2^k$ is a message and $\bs \getsr \zo^k$ is the secret key.
Encryption of $\bm$ is defined as $Enc_\bs(\bm) \defeq (\bA, \bc = (\bs^\top\bA \oplus \be \oplus \bm^\top\bG))$.
Here, $\bA \getsr \bbF_2^{k\times n}$, $\be \drawn \Ber_\tau^n$, and $\bG \in \bbF_2^{k\times n}$ is a public generator matrix of a binary linear code such that the code $C$ can correct any $\tau'\cdot n$ errors, for some $\tau' > \tau$.
Then to decrypt $\bm$, one computes $\bc' = \bc \oplus \bs^\top \bA$ and performs erasure recovery on $\bc'$ to recover the original codeword to recover $\bm$.
If the code $C$ can recover $\tau' n$ errors for $\tau' > \tau$, then with high probability this succeeds.

The intuition of this secret key encryption scheme is as follows.
We can use the $n$-bit sample of \LPN as a one-time pad for some encoding of the original message $\bm$.
This encoding must be able to correct at least $\tau n $ errors, which allows for recovery of $\bm$ after masking with $\bs^\top \bA \oplus \be$.
The decisional \LPN assumption gives that this encryption scheme is secure under chosen message attacks.

In this scenario, the scheme trades-off secret key size with message size.
Indeed, the above scheme as presented has secret key complexity $k$ and sample complexity $kn + nh_2(\tau)$ with message complexity $(k+1)n$.
The original scheme of \cite{ICALP:GilRobSeu08} presents $\bA$ as the shared secret key, where $\bs$ is sampled and the cipher text is $(\bs, \bs^\top \bA \oplus \be \oplus \bm^\top\bG)$.
Thus the message complexity is $k + n$ with sample complexity $k + nh_2(\tau)$ and secret key complexity $kn$.

It is mentioned that it is possible the scheme is still secure if $\bA$ is a Toeplitz matrix, thus having sample and description complexity $k+n$, benefiting both schemes above.
However it is not known if \LPN is still hard for the case that $\bA$ is a Toeplitz matrix.
Another possible direction would be to examine whether the scheme is still secure if the matrix $\bA$ were public, as in the case of the \LPN PRG and the binary linear code generated by $\bG$ above.
This would allow \bA to be sampled once and used multiple times (up to some limit).

\paragraph{Private Key Authentication and MACs.} \LPN-based private key authentication (or symmetric message authentication) are of interest because they have both provable security reductions and enjoy efficiency improvements over the number-theoretic approaches to the above.
Initiated by \cite{AC:HopBlu01}, their initial HB protocol is a 2 round symmetric message authentication protocol provably secure against passive adversaries assuming \LPN is hard.
Subsequently, \cite{C:JueWei05} improved the HB protocol, called HB$^+$, to active security at the cost of 3 rounds. 
However, the above protocol (and many other variants) were all susceptible to man-in-the-middle attacks.

\paragraph{Oblivious Transfer.} \cite{EPRINT:DGMN08a}





\subsubsection{Variants of \LPN}
We discuss two variants of \LPN and their results in this section.

\paragraph{Exact \LPN.} Exact \LPN, or \XLPN








